# -*- coding: utf-8 -*-
"""Internship Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Xhedj_s4MqA5Cq6QO1afMZD-XSSN3vY3
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression
import seaborn as sns                       #visualisation
from sklearn.model_selection import train_test_split
import os
import numpy as np
import matplotlib.pyplot as plt             #visualisation
# %matplotlib inline

my=pd.read_csv('personel_bank.csv')

my.head()

my.tail()

"""**1.Checking the type of data's and summary stats**"""

my.info()

my.describe()

"""**2. Dropping the unrequired columns**"""

my.isnull().sum()

my.Online.sum()

my.Experience

ex = my['Experience']
ag = my['Age']
cor = ex.corr(ag)
cor

my = my.drop(['ID','ZIP Code'],axis=1)
my.head()

my = my.dropna()
my.count()

print(my.isnull().sum())

"""**Finding the unique elements in the dataset**"""

my.apply(lambda x: len(x.unique()))

"""**Finding people with zero mortgage**"""

my[my['Mortgage'] == 0]['Mortgage'].count()

"""**Number of people with zero credit card spending per month**"""

my[my['CCAvg'] == 0]['CCAvg'].count()

plt.figure(figsize=(15,5))
plt.subplot(1,2,1)
sns.barplot(my['Personal Loan'], my['Age'])
plt.subplot(1,2,2)
sns.barplot(my['Personal Loan'], my['Experience'])

sns.boxplot(x=my['Income'])

sns.boxplot(x=my['Mortgage'], color='red')

sns.boxplot(x=my['CCAvg'], color='orange')

sns.boxplot(x=my['CD Account'], color='violet')

q1 = my.quantile(0.25)
q3 = my.quantile(0.75)
IQR = q3-q1
print(IQR)

df = df[~((df < (q1 - 1.5 * IQR)) | (df > (q3 + 1.5 * IQR ))).any(axis = 1)]
df.shape

c = my['Income'].value_counts()*100/sum(df['Income'].value_counts())
print(c)

pl = c.index[:10]

# plotting a bar graph of Income 
plt.figure(figsize = (10,5))
plt.barh(pl, width = c[:10])
plt.title('Income')
plt.show()

"""**Calculating Correlation matrix**"""

my.corr()

#plotting the heatmap for all the parameters currently present in the dataset
plt.figure(figsize=(10,5))
co = my.corr()
sns.heatmap(co,cmap = "BrBG",annot = True)

# plotting the point graph between Income and Age
fig, point = plt.subplots(figsize = (10,5))
point.scatter(df['Age'],df['Income'])
point.set_xlabel('Age')
point.set_ylabel('Income')
plt.show()

my.info()

my['Age'].value_counts().plot.bar(figsize = (10,6))
plt.title("Credit Card v/s Age")
plt.ylabel('No.of credit cards')
plt.xlabel('Age')

sns.countplot(y='Education', data = my , hue = 'Securities Account')
plt.title('Edu vs Securities Account')
plt.ylabel('Education')
plt.xlabel('Securities Account')

my.info()

x=my[['Age','Income','CCAvg','Education','Mortgage','Securities Account']]
y=my['Personal Loan'].values

from sklearn.preprocessing import StandardScaler
Sc_x = StandardScaler()
Sc_y = StandardScaler()
x = Sc_x.fit_transform(x)
y = Sc_y.fit_transform(y.reshape(-1,1))

train_x,test_x,train_y,test_y = train_test_split(x,y,test_size = 0.3, random_state = 0)

print(train_x.shape,test_x.shape,train_y.shape,test_y.shape)

my.head()

"""**3. The final Testing and Training Part**"""

fina = pd.read_csv('personel_bank.csv')

fina.head()

fina.info()

cols=['a','b','c','d','e','f','g','h','i','j','k','l','m']

fina.head()

fina.count()

features_selection=['Age','Income','CCAvg','Education']

x_a = fina[features_selection]

y_a = fina['Personal Loan']

train_x,test_x,train_y,test_y=train_test_split(x_a,y_a,random_state=0)
print(train_x.shape,test_x.shape,train_y.shape,test_y.shape)

"""Various types of algorithm

**1.Using the logistic Regression model to calculate the performance.**
"""

#Using the logistic Regression model to calculate the performance.
from sklearn.linear_model import LogisticRegression

model=LogisticRegression()
model.fit(train_x,train_y)

predict_y = model.predict(test_x)
print(predict_y)

sns.distplot((test_y - predict_y),bins = 50)

plt.scatter(test_y,predict_y)

"""**Claculating the mean absolute error, root mean square error, r2 score and the accuracy of the model.**"""

# Claculating the mean absolute error, root mean square error, r2 score and the accuracy of the model.
from sklearn import metrics
print('Mean Absolute Error is : ', metrics.mean_absolute_error(test_y,predict_y))
print('Root Mean Squared Error is : ', np.sqrt(metrics.mean_squared_error(test_y,predict_y)))
print('R2 Score is : ', metrics.r2_score(test_y,predict_y))
print('Accuracy score is : ', metrics.accuracy_score(test_y,predict_y))
print('The Confusion Matrix is : ')
print(metrics.confusion_matrix(test_y, predict_y))

"""**Making the classification report of the predicted model**"""

from sklearn.metrics import classification_report
# Making the classification report of the predicted model
print(classification_report(test_y,predict_y))

"""**Finding the recall score of the model**"""

# finding the recall score of the model
print('The recall score is : ', metrics.recall_score(test_y,predict_y))

plt.plot(predict_y)

"""**Using polynomial function for calculating the performance of the dataset.**"""

#using polynomial function for calculating the performance of the dataset.
from sklearn.preprocessing import PolynomialFeatures

polynom_reg = PolynomialFeatures(degree = 4)
poly_x = polynom_reg.fit_transform(train_x)
polynom_reg.fit(poly_x,train_y)
log_reg = LogisticRegression()
log_reg.fit(poly_x, train_y)

predict_y = log_reg.predict(polynom_reg.fit_transform(test_x))
plt.scatter(test_y,predict_y)

sns.distplot((test_y-predict_y),bins = 50)

"""**Claculating the mean absolute error, root mean square error, r2 score and the accuracy of the model.**"""

# Claculating the mean absolute error, root mean square error, r2 score and the accuracy of the model.
from sklearn import metrics
print('Mean Absolute Error is : ', metrics.mean_absolute_error(test_y,predict_y))
print('Root Mean Squared Error is : ', np.sqrt(metrics.mean_squared_error(test_y,predict_y)))
print('R2 Score is : ', metrics.r2_score(test_y,predict_y))
print('Accuracy score is : ', metrics.accuracy_score(test_y,predict_y))
A = model.score(test_x, test_y)
print("The accuracy of the Logistic regression model is : ", A)

"""The classification goal is to predict the likelihood of a liability customer buying personal loans.

**2. Naive bayes model**
"""

from sklearn.impute import SimpleImputer 
im = SimpleImputer(missing_values=np.nan, strategy='mean')
from sklearn import preprocessing
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score

X = fina.values[:,0:9]
Y = fina.values[:,10]

train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size = 0.30, random_state = 0)

cls = GaussianNB()
cls.fit(train_x, train_y)

predict_y = cls.predict(test_x)
B=accuracy_score(test_y, predict_y, normalize = True)
print("Naive bayes Accuracy is : ", B)
print("Naive bayes Confusion matrix is : ")
print(metrics.confusion_matrix(test_y,predict_y))

"""**3. KNN model**"""

from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler

X = pd.DataFrame(StandardScaler().fit_transform(fina))
X.columns = fina.columns

X = np.array(fina.iloc[:,1:11]) 
Y = np.array(fina['Personal Loan'])

train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.3, random_state=0)

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import recall_score

# instantiate learning model (k = 1)
k = KNeighborsClassifier(n_neighbors = 7)

# fitting the model
k.fit(train_X, train_Y)

# predict the response
predict_y = k.predict(test_X)

# evaluate accuracy
C=accuracy_score(test_Y, predict_y)   #Accuracy of KNN model
print('Accuracy_score:',C)  
print('Confusion_matrix:')
print(metrics.confusion_matrix(test_Y, predict_y))

"""**4. SVM Model**"""

from sklearn.model_selection import train_test_split

# To calculate the accuracy score of the model
from sklearn.metrics import accuracy_score, confusion_matrix

target = fina["Personal Loan"]
features=fina.drop(['ID','ZIP Code','Experience'], axis =1 )
X_train, X_test, y_train, y_test = train_test_split(features,target, test_size = 0.30, random_state = 10)

from sklearn.svm import SVC

# Building a Support Vector Machine on train data
svc_model= SVC(kernel='linear')
svc_model.fit(X_train, y_train)

prediction = svc_model.predict(X_test)

# check the accuracy on the training set
print(svc_model.score(X_train, y_train))
print(svc_model.score(X_test, y_test))

print("Confusion Matrix:\n",confusion_matrix(prediction,y_test))

#Store the accuracy results for each kernel in a dataframe for final comparison
resultsfina = pd.DataFrame({'Kernel':['Linear'], 'Accuracy': svc_model.score(X_train, y_train)})
resultsfina = resultsfina[['Kernel', 'Accuracy']]
resultsfina

svc_model  = SVC(kernel='poly')
svc_model.fit(X_train, y_train)

prediction = svc_model.predict(X_test)

print(svc_model.score(X_train, y_train))
print(svc_model.score(X_test, y_test))

#Store the accuracy results for each kernel in a dataframe for final comparison
tempResultsfina = pd.DataFrame({'Kernel':['Poly'], 'Accuracy': svc_model.score(X_train, y_train)})
resultsfina = pd.concat([resultsfina, tempResultsfina])
resultsfina = resultsfina[['Kernel', 'Accuracy']]
resultsfina

"""**Comparison of different Models:**"""

print(A) #Accuracy of Logistic regression model

print(B) #Accuracy of Naive Bayes' Model

print(C)  #Accuracy of KNN Model

resultsfina #Accuracy of SVM Model

"""As you can see the logistic model is giving an accuracy of 95.36% compared to other models for predicting the likelihood of a liability customer buying personal loans."""